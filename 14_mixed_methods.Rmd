# Mixed-methods research: Analysing qualitative data in *R* {#mixed-methods-research}

```{r, include=FALSE}
library(tidyverse)
library(r4np)
```

Conducting mixed-methods research is challenging for everyone. It requires an understanding of different methods and data types and particular knowledge in determining how one can mix different methods to improve the insights compared to a single method approach. This chapter looks at one possibility of conducting mixed-methods research in *R*. It is likely the most evident use of computational software for qualitative data.

While I consider myself comfortable in qualitative and quantitative research paradigms, this chapter could be somewhat uncomfortable if you are used to only one or the other approach, i.e. only quantitative or only qualitative research. However, with advancements in big data science, it is impossible to code, for example, two million tweets qualitatively. Thus, the presented methodologies should not be considered in isolation from other forms of analysis. For some, they might serve as a screening tool to sift through large amounts of data and find those nuggets of most significant interest that deserve more in-depth analysis. For others, these tools constitute the primary research design to allow to generalise to a larger population. In short: the purpose of your research will dictate the approach.

Analysing text quantitatively, though, is not new. The field of Corpus Analysis has been doing this for many years. If you happen to be a Corpus Analyst, then this section might be of particular interest.

This chapter will cover two analytical approaches for textual data:

-   Word frequencies, and

-   Word networks with n-grams.

## The tidy process of working with textual data {#tidy-process-for-textual-data}

Before we dive head-first into this exciting facet of research, we first need to understand how we can represent and work with qualitative data in *R*. Similar to working with tidy quantitative data, we also want to work with tidy qualitative data. This book adopts the notion of tidy text data from @silge2017text, which follows the terminology used in Corpus Linguistics:

> We (...) define the tidy text format as being **a table with one-token-per-row.** A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens.

In other words, what used to be an 'observation' is now called a 'token'. Therefore, a token is the smallest chosen unit of analysis. The term 'word token' can easily be confused with 'word type'. The first one usually represents the instance of a 'type'. Thus, the frequency of a word type is determined by the number of its tokens. For example, consider the following sentence, which consists of five tokens, but only four types:

```{r Example sentence for identifying tokes and word types, echo=TRUE}
(sentence <- "This car, is my car.")
```

Because the word `car` appears twice, we have two tokens of the word type `car` in our dataset. However, how would we represent this in a rectangular dataset? If each row represents one token, we would expect to have five rows in our data frame. As mentioned above, the process of converting a text into individual tokens is called 'tokenization'. To turn our text into a tokenized data frame, we have to perform two steps:

1.  Convert our `text` object into a data frame, and
2.  Split this text into individual words, i.e. tokenization.

While the first part can be achieved using `tibble()`, we need a new function to perform tokenisation. The package `tidytext` will be our primary tool of choice, and it comes with the function `unnest_tokens()`. Among many other valuable applications, `unnest_tokens()` can tokenize the text for us.

```{r Converting text into a data frame, echo=TRUE}
# Convert text object into a data frame
(df_text <- tibble(text = sentence))
```

```{r}
# Tokenization
library(tidytext)
(df_text <- df_text %>% unnest_tokens(output = word,
                                      input = text))
```

If you paid careful attention, two elements got lost in our data during the process of tokenization: the `.` and `,` are no longer included. In most cases, commas and full stops are of less interest because they tend to carry no particular meaning when performing such an analysis. The function `unnest_tokens()` conveniently removes them for us and also converts all letters to lower-case.

From here onwards we find ourselves in more familiar territory because the variable `word` looks like any other character variable we encountered before. For example, we can `count()` the number of occurrences for each word.

```{r Count words, echo=TRUE}
df_text %>% count(word)
```

Since we have our data nicely summarised, we can also easily visualise it using `ggplot()`.

```{r Visualise word frequencies, echo=TRUE}
df_text %>%
  count(word) %>%
  ggplot(aes(x = word,
             y = n)) +
  geom_col()
```

Once we have converted our data into a data frame, all the techniques we covered for non-quantitative variables can be applied. It only requires a single function to turn ourselves into novice corpus analysts. If this sounds too easy, then you are right. Hardly ever will we retrieve a tidy dataset that allows us to work with it in the way we just did. Data cleaning and wrangling still need to be performed but in a slightly different way. Besides, there are many other ways to tokenize text than using individual words, some of which we cover in this chapter.

## Stop words: Removing noise in the data

In the previous example, we already performed an important data wrangling process, i.e. tokenization. However, besides changing the text format, we also need to take care of other components in our data that are usually not important, for example, removing *'stop words'*. To showcase this step (and a little more), I will draw on the `imdb_top_250` dataset.

In marketing, finding the right product name is a laborious task and requires careful attention. After all, our first impression of a product is partially informed by its name. Consequently, when promoting a movie, one might wonder whether more popular words in movie titles can lead to greater success, i.e. a higher ranking on IMDb. With the help of the `imdb_top_250` dataset, we can empirically investigate this matter. However, we first have to tidy the data and then clean it. An essential step in this process is the removal of *'stop words'*.

*'Stop words'* are words that we want to exclude from our analysis, because they carry no particular meaning. The `tidytext` package comes with a data frame that contains common stop words in English.

```{r List of stop words, echo=TRUE}
stop_words
```

It is important to acknowledge that there is no unified standard of what constitutes a stop word. The data frame `stop_words` covers many words, but in your research context, you might not even want to remove them. For example, if we aim to identify topics in a text, stop words would be equivalent to white noise, i.e. many of them are of no help to identify relevant topics. Instead, stop words would confound our analysis. For obvious reasons, stop words highly depend on the language of your data. Thus, what works for English will not work for German, Russian or Chinese. You will need a separate set of stop words for each language.

The removal of stop words can be achieved with a function we already know: `anti-join()`. In other words, we want to subtract all words from `stop_words` from a given text. Let's begin with the tokenization of our movie titles.

```{r Tokenization of movie titles, echo=TRUE}
titles <-
  imdb_top_250 %>%
  unnest_tokens(word, title) %>%
  select(imdb_rating, word)

titles
```

Our new data frame has considerably more rows, i.e. `712`, which hints at around 3 words per title on average. If you are the curious type, like me, we already can peek at the frequency of words in our dataset.

```{r Frequency of words pre stop word removal, echo=TRUE}
titles %>%
  mutate(word2 = fct_lump_n(word, 5,
                            other_level = "other words")) %>%
  count(word2) %>%
  ggplot(aes(x = reorder(word2, n),
             y = n)) +
  geom_col() +
  coord_flip()
```

The result is somewhat disappointing. None of these words carries any particular meaning because they are all stop words. Thus, we must clean our data before conducting such an analysis.

I also sneaked in a new function called `fct_lump_n()` from the `forcats` package. It creates a new factor level called `other words` and 'lumps' together all the other factor levels. You likely have seen plots before which show a category called 'Other'. We usually apply this approach if we have many factor levels with very low frequencies. It would not be meaningful to plot 50 words as a barplot which only occurred once in our data. They are less important. Thus, it is sometimes meaningful to pool factor levels together. The function `fct_lump_n(word, 5)` returns the five most frequently occurring words and pools the other words together into a new category. There are many different ways to 'lump' factors. For a detailed explanation and showcase of all available alternatives, have a look at the [forcats website](https://forcats.tidyverse.org/reference/fct_lump.html "forcats website"){target="blank"}.

In our next step, we have to remove stop words using the `stop_words` data frame and apply the function `anti_join()`.

```{r Remove stop words, echo=TRUE}
titles_no_sw <- anti_join(titles, stop_words,
                          by = "word")

titles_no_sw
```

The dataset shrank from `712` rows to `443` rows. Thus, almost 38% of our data was actually noise. With this cleaner dataset, we can now look at the frequency count again.

```{r Word frequency after removal of stop words, echo=TRUE}
titles_no_sw %>%
  count(word) %>%
  filter(n > 1) %>%
  ggplot(aes(x = reorder(word, n),
             y = n)) +
  geom_col() +
  coord_flip()
```

To keep the number of bars somewhat manageable, I filtered those words with more than `1` occurrence in the dataset. Some of the most frequently occurring words include, for example, `wild`, `wars`, `story`, `star`, `rings`, `lords`. If you know Hollywood movies of the past, you will notice that two movies likely cause this result: The 'Star Wars' and 'Lord of the Rings' series. We also have numbers included in our dataset. Sometimes these are worth removing, but in this case, the number `2` likely indicates a sequel to another movie. Thus, it provides essential information.

With our data in place, it is time to perform our analysis. We want to know whether certain words are associated with higher `imdb_rating`s. If we use the function `count()` we will lose the variable `imdb_rating()`. Thus, we have to do this in three steps:

1.  We create the frequencies for each word with `count()`.

2.  Then, we `left_join()` the word frequencies with the original dataset to add the word frequencies.

3.  Compute a correlation using `correlation()` from the `correlation` package.

```{r Compute relationship between imdb rating and word frequency, echo=TRUE}
# Count the number of words
word_frequencies <-
  titles_no_sw %>%
  count(word)
```

```{r}
# Combine word_frequencies with titles_no_sw
titles_no_sw <- left_join(titles_no_sw, word_frequencies,
                          by = "word")

titles_no_sw
```

```{r}
# Compute the correlation of imdb_rating and n
corr <-
  titles_no_sw %>%
  select(imdb_rating, n) %>%
  correlation::correlation()

corr
```

```{r}
# Interpret the effect size
effectsize::interpret_r(corr$r, rules = "cohen1988")
```

Considering our correlation analysis, we find that more frequently used words tend to be significantly more successful. However, the effect size is too small to be of particular relevance. Besides, as mentioned before, the frequency of words is skewed towards two movie franchises, both of which have been very successful. Thus, we have to be content that there is no secret formula to create movie titles, i.e. choosing popular words from movie titles will not result in a better ranking.

## N-grams: Exploring correlations of words {#n-grams}

Besides looking at words in isolation, it is often more interesting to understand combinations of words to provide much-needed context. For example, the difference between 'like' and 'not like' can be crucial when trying to understand sentiments in data. The co-occurrence of words follows the same idea as correlations, i.e. how often one word appears together with another. If the frequency of word pairs is high, the relationship between these two words is strong. The technical term for looking at tokens that represent pairs for words is *'bigram'*. If we look at more than two words, we would consider them as *'n-gram'*, where the *'n'* stands for the number of words.

Creating a bigram is relatively simple in *R* and follows similar steps as counting word frequencies:

1.  Turn text into a data frame.
2.  Tokenize the variable that holds the text, i.e. `unnest_tokens()`,
3.  Split the two words into separate variables, e.g. `word1` and `word2`, using `separate()`.
4.  Remove stop words from both variables listwise, i.e. use `filter()` for `word1` and `word2`.
5.  Merge columns `word1` and `word2` into one column again, i.e. `unite()` them. (optional)
6.  Count the frequency of bigrams, i.e. `count()`.

There are a lot of new functions covered in this part. However, by now, you likely understand how they function already. Let's proceed step-by-step. For this final example about mixed-methods research, we will look at the `synopsis` of movies provided by IMDb. Since we already have the text in our data frame, we can skip step one. Next, we need to engage in tokenization. While we use the same function as before, we need to provide different arguments to retrieve bigrams. As before, we need to define an `output` column and an `input` column. In addition, we also have to provide the correct type of tokenization, i.e. determine `token`, which we need to set to `"ngrams"`. We also need to define the `n` in 'ngrams', which will be `2` for bigrams. After this, we apply `count()` to our variable `bigram`, and we achieved our task. If we turn this into *R* code, we get the following (with considerably less characters than its explanation):

```{r Tokenizing for ngrams, echo=TRUE}
# Create bigrams from variable synopsis
synopsis <-
  imdb_top_250 %>%
  unnest_tokens(bigram, synopsis, token = "ngrams", n = 2) %>%
  select(bigram)

synopsis
```

```{r}
# Inspect frequency of bigrams
synopsis %>% count(bigram, sort = TRUE)
```

As before, the most frequent bigrams are those that contain stop words. Thus, we need to clean our data and remove them. This time, though, we face the challenge that the variable bigram has two words and not one. Thus, we cannot simply use `anti_join()` because the dataset `stop_words` only contains individual words, not pairs. To remove stop words successfully, we have to `separate()` the variable into two variables so that each word has its own column. The package `tidyr` makes this is an effortless task.

```{r Split bigram into two separate columns, echo=TRUE}
bigram_split <- synopsis %>%
  separate(col = bigram,
           into = c("word1", "word2"),
           sep = " ")

bigram_split
```

With this new data frame, we can remove stop words in each variable. Of course, we could use `anti_join()` as before and perform the step twice, but there is a much more elegant solution. Another method to compare values across vectors/columns is the `%in%` operator. It is very intuitive to use because it tests whether values in the variable on the left of `%in%` exist in the variable to the right. Let's look at a simple example. Assume we have an object that contains the names of people related to `family` and `work`. We want to know whether people in our `family` are also found in the list of `work`. If you remember Table \@ref(tab:logical-operators-r), which lists all logical operators, you might be tempted to try `family == work`. However, this would assess the object in its entirety and not tell us which values, i.e. which names, can be found in both lists.

```{r Example of the use of in operator p1, echo=TRUE}
# Define the two
family <- tibble(name = c("Fiona", "Ida", "Lukas", "Daniel"))
work <- tibble(name = c("Fiona", "Katharina", "Daniel"))

# Compare the two objects using '=='
family$name == work$name
```

All but one value in `family` and `work` are not equal, i.e. they are `FALSE`. This is because `==` compares the values in order. If we changed the order, some of the `FALSE` values would turn `TRUE`. In addition, we get a warning telling us that these two objects are not equally long because `family` holds four names, while `work` contains only three names. Lastly, the results are not what we expected because `Fiona` and `Daniel` appear in both objects. Therefore we would expect that there should be two `TRUE` values. In short, `==` is not the right choice to compare these two objects.

If we use `%in%` instead, we can test whether each name appears in both objects, irrespective of their length and the order of the values.

```{r Example of the use of in operator p2, echo=TRUE}
# Compare the two objects using '%in%'
family$name %in% work$name
```

If we want *R* to return the values that are the same across both objects, we can ask: `which()` names are the same?

```{r Example of the use of in operator p3, echo=TRUE}
# Compare the two objects using '%in%'
(same_names <- which(family$name %in% work$name))
```

The numbers returned by `which()` refer to the position of the value in our object. In our case, the first value in `family` is `Fiona`, and the fourth value is `Daniel`. It is `TRUE` that these two names appear in both objects. If you have many values that overlap in a large dataset, you might not want to know the row number but retrieve the actual values. This can be achieved by `slice()`ing our dataset, i.e. filtering our data frame by providing row numbers.

```{r Example of the use of in operator p4, echo=TRUE}
# Retrieve the values which exist in both objects
family %>%
  slice(same_names)
```

While this might be a nice little exercise, it is important to understand how `%in%` can help us remove stop words. Technically, we try to achieve the opposite, i.e. we want to keep the values in `word1` and `word2` that are not a word in `stop_words`. If we use the language of `dplyr`, we `filter()` based on whether a word in `bigram_split` is not `%in%` the data frame `stop_words`. Be aware that `%in%` requires a variable and not an entire data frame to work as we intend.

```{r Removing stop words from bigram, echo=TRUE}
bigram_cleaned <-
  bigram_split %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_cleaned <- bigram_cleaned %>%
  count(word1, word2, sort = TRUE)

bigram_cleaned
```

The result is a clean dataset, which reveals that `world` and `war` are the most common bigram in the synopsis of our Top 250 IMDb movies.

Sometimes we might wish to re-`unite()` the two columns that we separated. For example, when plotting the results into a `ggplot()`, we need both words in one column[^mixed_methods-1] to use the actual bigrams as labels.

[^mixed_methods-1]: With `glue::glue()` you can also combine values from two different columns directly in `ggplot()`, but it seems simpler to just use `unite()`. Remember, there is always more than one way of doing things in R.

```{r Unite two columns into one for bigram, echo=TRUE}
bigram_cleaned %>%
  unite(col = bigram,
        word1, word2,
        sep = " ")
```

So far, we primarily looked at frequencies as numbers in tables or as bar plots. However, it is possible to create network plots of words with bigrams by drawing a line between `word1` and `word2`. Since there will be overlaps across bigrams, they would mutually connect and create a network of linked words.

I am sure you have seen visualisations of networks before but might not have engaged with them on a more analytical level. A network plot consists of *'nodes'*, which represent observations in our data, and *'edges'*, which represent the link between nodes. We need to define both to create a network.

Unfortunately, `ggplot2` does not enable us to easily create network plots, but the package `ggraph` offers such features using the familiar `ggplot2` syntax. Network plots are usually made using specific algorithms to arrange values in a network efficiently. Thus, if you want your plots to be reproducible, you have to `set.seed()` in advance. This way, random aspects of some algorithms are set constant.

There is one more complication, but with a simple solution. The function `ggraph()`, which is an equivalent to `ggplot()`, requires us to create a `graph` object that can be used to plot networks. The package `igraph` has a convenient function that produces a `graph_from_data_frame()`. As a final step we need to choose a layout for the network. This requires some experimentation. Details about different layouts and more for the `ggraph` package can be found on its [website](https://ggraph.data-imaginist.com "ggraph website"){target="blank"}.

```{r Visualising ngrams as network plots p1, echo=TRUE}
library(ggraph)

# Make plot reproducible
set.seed(1234)

# Create the special igraph object
graph <- igraph::graph_from_data_frame(bigram_cleaned)

# Plot the network graph
graph %>%
  ggraph(layout = "kk") +  # Choose a layout
  geom_edge_link() +       # Draw lines between nodes
  geom_node_point()        # Add node points
```

The result looks like a piece of modern art. Still, it is not easy to understand what the plot shows us. Thus, we need to remove some bigrams that are not so frequent. For example, we could remove those that only appear once.

```{r Visualising ngrams as network plots p2, echo=TRUE}
# Filter bigrams
network_plot <- bigram_cleaned %>%
  filter(n > 1) %>%
  igraph::graph_from_data_frame()

# Plot network plot
network_plot %>%
  ggraph(layout = "kk") +
  geom_edge_link() +
  geom_node_point()
```

It seems that many bigrams were removed. Thus, the network plot looks much smaller and less busy. Nevertheless, we cannot fully understand what this visualisation shows because there are no labels for each node. We can add them using `geom_node_text()`. I also recommend offsetting the labels with `vjust` (vertical adjustment) and `hjust` (horizontal adjustment), making them easier to read. To enhance the network visualisation further, we could colour the edges based on the frequency of each token.

```{r Visualising ngrams as network plots p3, echo=TRUE}
# Plot network plot
network_plot %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(col = factor(n))) +
  geom_node_point() +
  geom_node_text(aes(label = name),
                 vjust = 1,
                 hjust = 1)
```

The final plot shows that `war` and other military aspects, e.g. `soldiers`, `army` are very prevalent in our dataset. Besides, we also find some famous movie characters in our visualisation, such as `darth` `vader`, `luke` `skywalker` or `indiana` `jones`.

Our example is a simple analysis based on a relatively small 'corpus', i.e. a small dataset. However, the application of n-grams in larger datasets can reveal topic areas and links between them. There are more approaches to exploring topics in large datasets, for example, ['topic modelling'](https://www.tidytextmining.com/topicmodeling.html "Topic Modeling"){target="blank"}, which are far more complex, but offer more sophisticated analytical insights.

## Exploring more mixed-methods research approaches in *R*

This chapter can only be considered a teaser for mixed-methods research in *R*. There is much more to know and much more to learn. If this type of work is of interest, especially when working with social media data or historical text documents, there are several *R* packages I can recommend:

-   `tm`: This [text mining package](http://tm.r-forge.r-project.org "tm package"){target="blank"} includes many functions that help to explore documents systematically.

-   `quanteda`: This [package](https://quanteda.io "Quanteda"){target="blank"} offers natural language processing features and incorporates functions that are commonly seen in corpus analysis. It also provides tools to visualise frequencies of text, for example, wordclouds.

-   `topicmodels`: To analyse topics in large datasets, it is necessary to use special Natural Language Processing techniques. This [package](https://cran.r-project.org/web/packages/topicmodels/index.html "topicmodels package"){target="blank"} offers ways to perform *Latent Dirichlet Allocation (LDA)* and *Correlated Topic Models (CTM)*. Both are exciting and promising pathways to large-scale text analysis.

-   The `tidytext` package we used above also has a lot more to offer than what we covered, for example, performing *sentiment analyses*.

If you want to explore the world of textual analysis in greater depth, there are two fantastic resources I wholeheartedly can recommend:

-   [Text mining with tidy data principles](https://juliasilge.shinyapps.io/learntidytext/ "Text mining with tidy data principles"){target="blank"}: This free online course introduces additional use cases of the tidytext package by letting you work with data interactively right in your browser. Julia Silge developed these materials.

-   [Supervised Machine Learning for Text Analysis in R](https://smltar.com "Supervised Machine Learning for Text Analysis in R"){target="blank"}: This book is the ideal follow-up resource if you worked your way through @silge2017text. It introduces the foundations of natural language analysis and covers advanced text analysis methods such as regression, classification, and deep learning.

I hope I managed to showcase the versatility of *R* to some degree in this chapter. Needless to say, there is also a lot I still have to learn, which makes using *R* exciting and, more than once, has inspired me to approach my data in unconventional ways.
